{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74410f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1.1 Load training dataset\n",
    "training_set = pd.read_csv(\"DIA_trainingset_RDKit_descriptors.csv\")\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = training_set.iloc[:, 2:]\n",
    "Ytrain = training_set.iloc[:, 0]\n",
    "\n",
    "# Display label distribution\n",
    "print(Ytrain.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677eb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Load test dataset\n",
    "test_set = pd.read_csv(\"DIA_testset_RDKit_descriptors.csv\")\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b13222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    90\n",
      "1    30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Xtest = test_set.iloc[:, 2:]\n",
    "Ytest = test_set.iloc[:, 0]\n",
    "\n",
    "# Display label distribution\n",
    "print(Ytest.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0374d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "Training set: 0\n",
      "Test set: 0\n",
      "\n",
      "Features reduced from 196 to 140\n"
     ]
    }
   ],
   "source": [
    "# 2. Feature preprocessing pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Preprocess features: standardization, variance threshold, correlation filtering\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features\n",
    "        X_test (pd.DataFrame): Test features\n",
    "        \n",
    "    Returns:\n",
    "        X_train_processed, X_test_processed (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    # 2.1. Check missing values\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(\"Training set:\", X_train.isnull().sum().sum())\n",
    "    print(\"Test set:\", X_test.isnull().sum().sum())\n",
    "    \n",
    "    # 2.2. Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    X_test_std = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    joblib.dump(scaler, 'scaler.pkl')  # Save the scaler for reproducibility\n",
    "    \n",
    "    # 2.3. Remove zero-variance features\n",
    "    selector = VarianceThreshold()\n",
    "    selector.fit(X_train_std)\n",
    "    keep_vars = X_train.columns[selector.variances_ != 0].tolist()\n",
    "    \n",
    "    X_train_var = X_train_std[keep_vars]\n",
    "    X_test_var = X_test_std[keep_vars]\n",
    "    \n",
    "    # 2.4. Remove highly correlated features\n",
    "    corr_matrix = X_train_var.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    drop_features = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "    \n",
    "    X_train_processed = X_train_var.drop(columns=drop_features)\n",
    "    X_test_processed = X_test_var.drop(columns=drop_features)\n",
    "    \n",
    "    print(f\"\\nFeatures reduced from {X_train.shape[1]} to {X_train_processed.shape[1]}\")\n",
    "    \n",
    "    # Save processed features\n",
    "    X_train_processed.to_csv(\"X_train_processed2.csv\", index=False)\n",
    "    X_test_processed.to_csv(\"X_test_processed2.csv\", index=False)\n",
    "    \n",
    "    return X_train_processed, X_test_processed\n",
    "\n",
    "\n",
    "# Preprocess features\n",
    "X_train_processed, X_test_processed = preprocess_features(Xtrain, Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a30342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8ce63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
